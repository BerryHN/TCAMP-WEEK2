{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = r'essay_train.txt'\n",
    "test_path = r'essay_test.txt'\n",
    "df_train = pd.read_table(train_path,header = None,names=[\"id\", \"text\", \"score1\", \"score2\",\"score\"])\n",
    "df_test = pd.read_table(test_path,header = None,names=[\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet \n",
    "import sys, nltk, re\n",
    "\n",
    "def tokenize(line):\n",
    "    \"\"\" Tokenizes  a string\n",
    "    Input: string to be tokenized\n",
    "    Output: list of tokens \"\"\"\n",
    "    \n",
    "    orig = line.strip()\n",
    "    return nltk.word_tokenize(orig)\n",
    "\n",
    "def pos(tokens):\n",
    "    \"\"\" PoS tags a list of tokens\n",
    "    Input: list of tokens to be PoS-tagged\n",
    "    Output: list of tuples with (token, PoS-tag)\"\"\"\n",
    "    \n",
    "    return nltk.pos_tag(tokens)\n",
    "    \n",
    "def penn_2_wordnet(pos_tag):\n",
    "    \"\"\" Converts Penn PoS-tags to Wordnet-style pos tags\n",
    "    Input: Penn pos tag\n",
    "    Output: wordnet Pos tag\"\"\"\n",
    "    \n",
    "    if pos_tag.startswith('N'): \n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    # eliminate punctuation\n",
    "    elif re.match('[\\.:,\\(\\)]', pos_tag):\n",
    "        return None\n",
    "    # default to noun\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def get_lemmas(token_tuples):\n",
    "    \"\"\" Lemmatizes a list of token-pos tuples\n",
    "    Input: list of token-pos tuples using the Penn tagset\n",
    "    Output: list of tokens\"\"\"\n",
    "    \n",
    "    lemmas = []\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for token_tuple in token_tuples:\n",
    "        # convert penn pos-tags to wordnet pos-tags\n",
    "        pos = penn_2_wordnet(token_tuple[1])\n",
    "        if pos: \n",
    "            lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "        \n",
    "    return lemmas\n",
    "    \n",
    "def count_lemmas(lemma_list):\n",
    "    \"\"\" Count number of unique lemmas\n",
    "    Input: list of lemmas\n",
    "    Output: number of unique lemmas\"\"\"\n",
    "    \n",
    "    return len(set(lemma_list))\n",
    "    \n",
    "    \n",
    "def calculate(essay):\n",
    "    \n",
    "    lemma_list = []\n",
    "    #token_count = 0.0\n",
    "    \n",
    "    tokens = tokenize(str(essay).lower())\n",
    "    #token_count = len(tokens)\n",
    "    posd = pos(tokens)       \n",
    "    lemmas = get_lemmas(posd)\n",
    "    lemma_list += lemmas\n",
    "    \n",
    "    lcount = count_lemmas(lemma_list)\n",
    "    #return float(lcount) / token_count\n",
    "    return lcount\n",
    "    \n",
    "def poslist(essay):\n",
    "    tokens = tokenize(str(essay).lower())\n",
    "    poslist = []\n",
    "    p = pos(tokens)\n",
    "    for pp in p:\n",
    "        poslist.append(pp[1])\n",
    "        \n",
    "    return ' '.join(poslist)\n",
    "    \n",
    "def comma_count(essay):\n",
    "    \n",
    "    return poslist(essay).count(',')\n",
    "\n",
    "def posngram(essay):\n",
    "    #POS-tag essay\n",
    "    tokens = tokenize(str(essay).lower())\n",
    "    posd = pos(tokens)\n",
    "    \n",
    "    # unigrams\n",
    "    ugs = set()\n",
    "    for p in posd:\n",
    "        ugs.add(p[1])\n",
    "        \n",
    "    # bigrams\n",
    "    bgs = set()\n",
    "    for i in range(0, len(posd)-1):\n",
    "        bi = (posd[i][1], posd[i+1][1])\n",
    "        bgs.add(bi)\n",
    "\n",
    "    # trigrams\n",
    "    tgs = set()\n",
    "    for i in range(0, len(posd)-2):\n",
    "        tgs.add((posd[i][1], posd[i+1][1], posd[i+2][1]))\n",
    "\n",
    "    return ugs, bgs, tgs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# getting binarized pos-ngrams    \n",
    "pos = df_train.text.apply(poslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(lowercase=False, ngram_range=(1,3), binary=True)\n",
    "ngs = cv.fit_transform(list(pos))\n",
    "posngrams = ngs.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = posngrams[:,np.where(posngrams.sum(axis=0)>10)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 3888)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posngrams[:,np.where(posngrams.sum(axis=0)>10)[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test = df_test.text.apply(poslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngs_test = cv.transform(pos_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550, 10404)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 10404)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posngrams.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = ngs_test[:,np.where(posngrams.sum(axis=0)>10)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pickle\n",
    "from textblob import Word\n",
    "from nltk import ngrams\n",
    "from textstat.textstat import textstat\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "def get_sat_word(path):\n",
    "    with open(path, 'r') as f:\n",
    "        sat = f.read()\n",
    "    sat_words = []\n",
    "    for sentence in sat.split('\\n'):\n",
    "        if len(sentence) < 1:\n",
    "            continue\n",
    "        sat_words.append(sentence.split()[0])\n",
    "    return sat_words\n",
    "\n",
    "\n",
    "# 1）长度相关\n",
    "def form_counts(content):\n",
    "    \"\"\"\n",
    "    :param sentence:\n",
    "    :return: 单词数word_count,句子数sentence_count,每个句子的平均单词数avg_sentence_len\n",
    "              拼写错误单词数spelling_errors,长单词数long_word\n",
    "    \"\"\"\n",
    "    char_count = len(content)\n",
    "    words = nltk.word_tokenize(content.strip())  # 会自动滤除标点符号/lower\n",
    "    word_count = len(words)  # 单词数\n",
    "\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sents = sent_detector.tokenize(content.strip())\n",
    "    sentence_count = len(sents)  # 句子数\n",
    "\n",
    "    if sentence_count != 0:\n",
    "        avg_sentence_len = word_count / sentence_count  # 每个句子的平均单词数\n",
    "    else:\n",
    "        avg_sentence_len = 0\n",
    "\n",
    "    spelling_errors = sum([Word(word).spellcheck()[0][0] != word for word in words])  # 拼写错误的单词数\n",
    "    long_word = sum([len(word) >= 7 for word in words])  # 长单词数\n",
    "\n",
    "    exc_count = words.count('!')\n",
    "    que_count = words.count('?')\n",
    "\n",
    "    stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    sat_words = get_sat_word('SAT_words')\n",
    "    stopwords_num = 0\n",
    "    sat_num = 0\n",
    "    for word in words:\n",
    "        if word in stopwords:\n",
    "            stopwords_num += 1\n",
    "        if word in sat_words:\n",
    "            sat_num += 1\n",
    "\n",
    "    return char_count, word_count, sentence_count, avg_sentence_len, spelling_errors, \\\n",
    "           long_word, exc_count, que_count, stopwords_num, sat_num\n",
    "\n",
    "\n",
    "# 2) language model counts\n",
    "def ngrams_counts(content):\n",
    "    words = nltk.word_tokenize(content.strip())  # 会自动滤除标点符号/lower\n",
    "    sents = \" \".join(words)\n",
    "\n",
    "    unigrams = [grams for grams in ngrams(sents.split(), 1)]\n",
    "    bigrams = [grams for grams in ngrams(sents.split(), 2)]\n",
    "    trigram = [grams for grams in ngrams(sents.split(), 3)]\n",
    "\n",
    "    unigrams_count = len([(item[0], unigrams.count(item)) for item in sorted(set(unigrams))])\n",
    "    bigrams_count = len([(item, bigrams.count(item)) for item in sorted(set(bigrams))])\n",
    "    trigrams_count = len([(item, trigram.count(item)) for item in sorted(set(trigram))])\n",
    "\n",
    "    return unigrams_count, bigrams_count, trigrams_count\n",
    "\n",
    "\n",
    "# 3) POS counts\n",
    "def pos_counts(content):\n",
    "    noun_count, adj_count, adv_count, verb_count, fw_count = 0, 0, 0, 0, 0\n",
    "\n",
    "    words = nltk.word_tokenize(content)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    for tag in tags:\n",
    "        if tag[1].startswith(\"NN\"):\n",
    "            noun_count += 1\n",
    "        elif tag[1].startswith(\"JJ\"):\n",
    "            adj_count += 1\n",
    "        elif tag[1].startswith(\"RB\"):\n",
    "            adv_count += 1\n",
    "        elif tag[1].startswith(\"VB\"):\n",
    "            verb_count += 1\n",
    "        elif tag[1].startswith(\"FW\"):\n",
    "            fw_count += 1\n",
    "    return noun_count, adj_count, adv_count, verb_count, fw_count\n",
    "\n",
    "\n",
    "# # 4) getFeature\n",
    "# def get_dataframe(content):\n",
    "#     features = pd.DataFrame()\n",
    "\n",
    "#     features['word_count'],features['sentence_count'],features['avg_sentence_len'],\\\n",
    "#     features['spelling_errors'],features['long_word'] = form_counts(content)\n",
    "\n",
    "#     features['unigrams_count'],features['bigrams_count'],features['trigrams_count'] = ngrams_counts(content)\n",
    "#     features['noun_count'], features['adj_count'], features['adv_count'],features['verb_count'], features['fw_count'] = pos_counts(content)\n",
    "\n",
    "#     return features\n",
    "def sentiment_tagger(sid = SentimentIntensityAnalyzer(),sent_detector = nltk.data.load('tokenizers/punkt/english.pickle'),essay = None):\n",
    "    neg_sentiment,neu_sentiment,pos_sentiment = 0, 0 ,0\n",
    "    sents = sent_detector.tokenize(essay.strip())\n",
    "\n",
    "    for sent in sents:\n",
    "        ss = sid.polarity_scores(sent)\n",
    "        for k in sorted(ss):\n",
    "            if k == 'neg':\n",
    "                neg_sentiment += ss[k]\n",
    "            elif k == 'neu':\n",
    "                neu_sentiment += ss[k]\n",
    "            elif k == 'pos':\n",
    "                pos_sentiment += ss[k]\n",
    "    return neg_sentiment,neu_sentiment,pos_sentiment\n",
    "\n",
    "def Readability(df_train):\n",
    "    syllable_count = df_train.text.apply(textstat.syllable_count)\n",
    "    flesch_reading_ease = df_train.text.apply(textstat.flesch_reading_ease)\n",
    "    smog_index = df_train.text.apply(textstat.smog_index)\n",
    "    flesch_kincaid_grade = df_train.text.apply(textstat.flesch_kincaid_grade)\n",
    "    coleman_liau_index = df_train.text.apply(textstat.coleman_liau_index)\n",
    "    automated_readability_index = df_train.text.apply(textstat.automated_readability_index)\n",
    "    dale_chall_readability_score = df_train.text.apply(textstat.dale_chall_readability_score)\n",
    "    difficult_words = df_train.text.apply(textstat.difficult_words)  # 困难单词数\n",
    "    linsear_write_formula = df_train.text.apply(textstat.linsear_write_formula)\n",
    "    gunning_fog = df_train.text.apply(textstat.gunning_fog)\n",
    "    return np.c_[syllable_count, flesch_reading_ease, smog_index, flesch_kincaid_grade, coleman_liau_index,\n",
    "                 automated_readability_index,dale_chall_readability_score,difficult_words,linsear_write_formula,gunning_fog]\n",
    "# 4) getFeature\n",
    "def get_features(df_train):\n",
    "    contents = list(df_train.text)\n",
    "    read = Readability(df_train)\n",
    "    features = []\n",
    "    for content in contents:\n",
    "        char_count, word_count, sentence_count, avg_sentence_len, spelling_errors, \\\n",
    "        long_word, exc_count, que_count, stopwords_num, sat_num = form_counts(content)\n",
    "        unigrams_count, bigrams_count, trigrams_count = ngrams_counts(content)\n",
    "        noun_count, adj_count, adv_count, verb_count, fw_count = pos_counts(content)\n",
    "        neg_sentiment, neu_sentiment, pos_sentiment = sentiment_tagger(content)\n",
    "        feature = np.c_[char_count, word_count, sentence_count, avg_sentence_len, spelling_errors,\n",
    "                        long_word, exc_count, que_count, stopwords_num, sat_num, unigrams_count,\n",
    "                        bigrams_count, trigrams_count, noun_count, adj_count, adv_count, verb_count, fw_count,\n",
    "                        neg_sentiment, neu_sentiment, pos_sentiment,]\n",
    "        features.append(feature[0])\n",
    "    return np.c_[features,read]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "read_train = Readability(df_train)\n",
    "senti_train = np.array([sentiment_tagger(essay=essay) for essay in df_train.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "read_test = Readability(df_test)\n",
    "senti_test = np.array([sentiment_tagger(essay=essay) for essay in df_test.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bi_features= np.c_[read_train,senti_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('readp.pickle','wb') as f:\n",
    "    pickle.dump([read_train,senti_train,read_test,senti_test],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.38000000e+02,   7.81400000e+01,   8.80000000e+00, ...,\n",
       "          5.13900000e+00,   2.45840000e+01,   2.27700000e+00],\n",
       "       [  5.72000000e+02,   5.64500000e+01,   1.23000000e+01, ...,\n",
       "          2.63900000e+00,   2.09500000e+01,   1.41000000e+00],\n",
       "       [  3.24000000e+02,   5.17200000e+01,   1.33000000e+01, ...,\n",
       "          6.34000000e-01,   4.08000000e+00,   2.87000000e-01],\n",
       "       ..., \n",
       "       [  4.31000000e+02,   7.49000000e+01,   9.10000000e+00, ...,\n",
       "          5.65000000e-01,   2.06690000e+01,   2.76600000e+00],\n",
       "       [  5.84000000e+02,   8.31500000e+01,   8.70000000e+00, ...,\n",
       "          4.09000000e-01,   2.78400000e+01,   4.75100000e+00],\n",
       "       [  7.84000000e+02,   8.64400000e+01,   8.80000000e+00, ...,\n",
       "          1.11600000e+00,   2.85000000e+01,   4.38500000e+00]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = get_features(list(df_train.text))\n",
    "test_features = get_features(list(df_test.text))\n",
    "with open('data.pickle','wb') as f:\n",
    "    pickle.dump([train_features,test_features], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "# Import all of the scikit learn stuff\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import pandas as pd\n",
    "import warnings\n",
    "# Suppress warnings from pandas library\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning,\n",
    "module=\"pandas\", lineno=570)\n",
    "\n",
    "def compute(df):\n",
    "    \n",
    "    \"\"\"LSA computation\n",
    "    \n",
    "    Input: pandas dataframe\n",
    "    Output: score of the most similar essay to the first essay\n",
    "    \"\"\"\n",
    "    \n",
    "    # get file's index\n",
    "    ind = df.name  \n",
    "    es = df[0]\n",
    "    essay = df[1]\n",
    "    \n",
    "    trainfiles = \\\n",
    "    pd.DataFrame.from_csv('data/training_set_rel3.tsv',\\\n",
    "    sep=\"\\t\")\n",
    "    # drop test file from training if present\n",
    "    trainfiles.drop(ind, inplace=True)\n",
    "    \n",
    "    scores = trainfiles[trainfiles.essay_set == es].domain1_score\n",
    "    # create training matrix\n",
    "    testfile = pd.Series(essay)    \n",
    "    trainfiles = trainfiles[trainfiles.essay_set == es].essay\n",
    "    data = testfile.append(trainfiles)\n",
    "    \n",
    "    \n",
    "    # create DTM\n",
    "    vectorizer = CountVectorizer(min_df = 1, stop_words = 'english')\n",
    "    dtm = vectorizer.fit_transform(data)\n",
    "        \n",
    "    \n",
    "    # Fit LSA\n",
    "    lsa = TruncatedSVD(100, algorithm = 'randomized')\n",
    "    dtm_lsa = lsa.fit_transform(dtm)\n",
    "    dtm_lsa = Normalizer(copy=False).fit_transform(dtm_lsa)\n",
    "    \n",
    "    similarity = np.asarray(np.asmatrix(dtm_lsa) * np.asmatrix(dtm_lsa).T)\n",
    "    simdf = pd.DataFrame(similarity,index=data.index, columns=data.index)\n",
    "    \n",
    "    closest_id = simdf.iloc[0][1:].idxmax()\n",
    "    \n",
    "    return scores[closest_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import  train_test_split\n",
    "from sklearn.metrics import  mean_squared_error\n",
    "from sklearn.svm import  SVR\n",
    "from scipy.stats import pearsonr\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def model_random_forest(Xtrain,Xtest,y_train,add):\n",
    "    X_train = Xtrain\n",
    "    # print(len(X_train[0]))\n",
    "    rfr = RandomForestRegressor(n_jobs=-1, random_state=10,oob_score=True)\n",
    "    param_grid ={'n_estimators': [300]}\n",
    "    #'n_estimators': [300]\n",
    "    # 'n_estimators': list(r,ange(30,60130)), ,\"max_depth\":list(range(40,61,10)),\"max_features\":list(range(120,211,30)),min_samples_split':list(range(20,201,20)),  'min_samples_leaf':list(range(10,80,10))\n",
    "    model = GridSearchCV(estimator=rfr, param_grid=param_grid, scoring= None, iid=False, cv=10)\n",
    "    model.fit(X_train, y_train)\n",
    "    # model.grid_scores_, model.best_params_, model.best_score_\n",
    "#     print('Random forecast classifier...')\n",
    "#     print('Best Params:')\n",
    "#     print(model.best_params_)\n",
    "#     print('Best CV Score:')\n",
    "    print(-model.best_score_)\n",
    "    y_pred = model.predict(Xtest)\n",
    "    final1 = model.predict(add)\n",
    "    return y_pred, final1\n",
    "\n",
    "\n",
    "\n",
    "def model_GBR(Xtrain,Xtest,y_train,add):\n",
    "    X_train = Xtrain\n",
    "    rfr = GradientBoostingRegressor(random_state=10,learning_rate= 0.01,loss= \"ls\")\n",
    "    param_grid = {'n_estimators': [300]}\n",
    "    # 'n_estimators': list(range(30,601,30)), ,\"max_depth\":list(range(40,61,10)),\"max_features\":list(range(120,211,30)),min_samples_split':list(range(20,201,20)),  'min_samples_leaf':list(range(10,80,10))\n",
    "    model = GridSearchCV(estimator=rfr, param_grid=param_grid, scoring= None, iid=False, cv=10)\n",
    "    model.fit(X_train, y_train)\n",
    "#     print('Best Params:')\n",
    "#     print(model.best_params_)\n",
    "    y_pred = model.predict(Xtest)\n",
    "    final2 = model.predict(add)\n",
    "    return y_pred,final2\n",
    "\n",
    "def model_XG(Xtrain,Xtest,y_train,add):\n",
    "    clf = XGBRegressor(nthread=4,n_estimators=100)\n",
    "    clf.fit(Xtrain,y_train)\n",
    "    y_pred = clf.predict(Xtest)\n",
    "    final3 = clf.predict(add)\n",
    "    return y_pred,final3\n",
    "\n",
    "def model_cv(Xtrain,Xtest,y_train,add):\n",
    "    lscv = LassoCV()\n",
    "    lscv.fit(Xtrain, y_train)\n",
    "    y_pred = lscv.predict(Xtest)\n",
    "    final = lscv.predict(add)\n",
    "    return y_pred,final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train.pickle','rb') as f:\n",
    "    train_feature = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('test.pickle','rb') as f:\n",
    "    test_feature = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('readp.pickle','rb') as f:\n",
    "    read_train,senti_train,read_test,senti_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for feature in train_feature:\n",
    "    X_train.append(feature[0])\n",
    "X_train = np.c_[np.array(X_train),read_train,senti_train,n_train]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for feature in test_feature:\n",
    "    X_test.append(feature[0])\n",
    "X_test = np.c_[np.array(X_test),read_test,senti_test,n_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 3919)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = np.c_[list(df_train.score1),list(df_train.score2),list(df_train.score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_cross, y_train, y_cross = train_test_split(X_train, scores, test_size=0.002, random_state=1021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = x_train\n",
    "xc = x_cross\n",
    "xt = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.359137152385944\n",
      "-0.39098759593029053\n",
      "The peasorn of 1: 0.828749112158\n",
      "-0.4475611309667319\n",
      "-0.5037494967318691\n",
      "The peasorn of 2: 0.835093778203\n",
      "-0.4899631890206925\n",
      "-0.5232950867109987\n",
      "The peasorn of 3: 0.84674229042\n",
      "-0.5052027589842714\n",
      "-0.5312392061833446\n",
      "The peasorn of 4: 0.846249684844\n",
      "-0.5170205789412818\n",
      "-0.5520558223458463\n",
      "The peasorn of 5: 0.84760534815\n",
      "-0.5383899357175749\n",
      "-0.5687403426672073\n",
      "The peasorn of 6: 0.863641245876\n",
      "-0.5441106520046867\n",
      "-0.5690561806968409\n",
      "The peasorn of 7: 0.861951045943\n",
      "-0.5478877156657099\n",
      "-0.5690759810444311\n",
      "The peasorn of 8: 0.86361081294\n",
      "-0.54930077679352\n",
      "-0.5768808625267485\n",
      "The peasorn of 9: 0.866296758604\n",
      "-0.554202054555064\n",
      "-0.5801328848827634\n",
      "The peasorn of 10: 0.867692323853\n",
      "-0.5684492887253303\n",
      "-0.6041742094671918\n",
      "The peasorn of 11: 0.868707499536\n",
      "-0.5716730391925569\n",
      "-0.6080853421869297\n",
      "The peasorn of 12: 0.868965211733\n",
      "-0.572780918169934\n",
      "-0.6080551568671507\n",
      "The peasorn of 13: 0.869272730256\n",
      "-0.5666871005888277\n",
      "-0.6155902084481253\n",
      "The peasorn of 14: 0.868265859238\n",
      "-0.5727830500903645\n",
      "-0.615797390699792\n",
      "The peasorn of 15: 0.86757253163\n",
      "-0.573257024738333\n",
      "-0.6189427095708295\n",
      "The peasorn of 16: 0.869201196372\n",
      "-0.5715083142087193\n",
      "-0.6150740799329315\n",
      "The peasorn of 17: 0.868315900303\n",
      "-0.571250584642336\n",
      "-0.6139339956994883\n",
      "The peasorn of 18: 0.868983098143\n",
      "-0.5730788878954569\n",
      "-0.6161127723963742\n",
      "The peasorn of 19: 0.869585139791\n",
      "-0.5716322914033206\n",
      "-0.6154737652535851\n",
      "The peasorn of 20: 0.868924326068\n",
      "-0.5743252178481327\n",
      "-0.6154481481754557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The peasorn of 21: 0.867625663124\n",
      "-0.5745620048940265\n",
      "-0.6130424281650751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The peasorn of 22: 0.867004060091\n",
      "-0.5768449532954115\n",
      "-0.6155079815476776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The peasorn of 23: 0.868372152156\n",
      "-0.5786852768577713\n",
      "-0.6167489409035379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The peasorn of 24: 0.869079366671\n",
      "-0.5848063545612756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6167298490468779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The peasorn of 25: 0.868295600025\n",
      "-0.5839415210051492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6213424206383312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The peasorn of 26: 0.865956674464\n",
      "-0.5813747791810947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6241033515059256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The peasorn of 27: 0.865952534865\n",
      "-0.5823547000443927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6246870050068856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The peasorn of 28: 0.865863315365\n",
      "-0.5834985922239055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6259406055670576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The peasorn of 29: 0.865764555132\n",
      "-0.5793672713779243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6276792951534975\n",
      "The peasorn of 30: 0.866118545546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/home/liuzhiqiang/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "p = []\n",
    "for i in range(1,x.shape[1]):\n",
    "    xtrain, xcross,xtest = x[:,:i],xc[:,:i],xt[:,:i]\n",
    "    y_pred, final1_1= model_random_forest(xtrain,xcross, y_train[:,0],xtest)\n",
    "    Y_pred, final1_2 = model_GBR(xtrain,xcross, y_train[:,0],xtest)\n",
    "    xg_pred, final1_3 = model_XG(xtrain,xcross, y_train[:,0],xtest)\n",
    "    cv_pred, final1_4 = model_cv(xtrain,xcross, y_train[:,0],xtest)\n",
    "    y_pred2, final2_1= model_random_forest(xtrain,xcross, y_train[:,1],xtest)\n",
    "    Y_pred2, final2_2 = model_GBR(xtrain,xcross, y_train[:,1],xtest)\n",
    "    xg_pred2, final2_3 = model_XG(xtrain,xcross, y_train[:,1],xtest)\n",
    "    cv_pred2, final2_4 = model_cv(xtrain,xcross, y_train[:,1],xtest)\n",
    "    print(\"The peasorn of %d:\"%i, pearsonr((Y_pred+Y_pred2+y_pred+y_pred2+xg_pred+xg_pred2+cv_pred+cv_pred2)/4, y_cross[:,2])[0])\n",
    "    p.append(pearsonr((Y_pred+Y_pred2+y_pred+y_pred2+xg_pred+xg_pred2+cv_pred+cv_pred2)/4, y_cross[:,2])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6023247312041085\n",
      "-0.6285062209453064\n",
      "The peasorn of 18: 0.983181533596\n"
     ]
    }
   ],
   "source": [
    "xtrain, xcross,xtest = x,xc,xt\n",
    "y_pred, final1_1= model_random_forest(xtrain,xcross, y_train[:,0],xtest)\n",
    "Y_pred, final1_2 = model_GBR(xtrain,xcross, y_train[:,0],xtest)\n",
    "xg_pred, final1_3 = model_XG(xtrain,xcross, y_train[:,0],xtest)\n",
    "cv_pred, final1_4 = model_cv(xtrain,xcross, y_train[:,0],xtest)\n",
    "y_pred2, final2_1= model_random_forest(xtrain,xcross, y_train[:,1],xtest)\n",
    "Y_pred2, final2_2 = model_GBR(xtrain,xcross, y_train[:,1],xtest)\n",
    "xg_pred2, final2_3 = model_XG(xtrain,xcross, y_train[:,1],xtest)\n",
    "cv_pred2, final2_4 = model_cv(xtrain,xcross, y_train[:,1],xtest)\n",
    "print(\"The peasorn of 18:\", pearsonr((Y_pred+Y_pred2+y_pred+y_pred2+xg_pred+xg_pred2+cv_pred+cv_pred2)/4, y_cross[:,2])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 306, 1131]),)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(x.sum(axis=0)==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   0,   0, ..., 959, 959, 959]),\n",
       " array([   6,    7,   17, ..., 3916, 3917, 3918]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(x==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGQcEMzMDHBDMzCzjgGBmZgD8f32mS0dSLBTJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f09c9f5d358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pylab import * \n",
    "x=range(len(p))\n",
    "plt.plot(x, p, 'ro-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The peasorn of all: 0.983181533596\n"
     ]
    }
   ],
   "source": [
    "# print(\"The MSE of RandomForest:\",mean_squared_error(y_pred,y_cross))\n",
    "# print(\"The MSE of GradientBoostingRegression:\", mean_squared_error(Y_pred, y_cross))\n",
    "# print(\"The MSE of XGboost\", mean_squared_error(xg_pred, y_cross))\n",
    "# print(\"The Peasorn of RandomForest:\", pearsonr(y_pred, y_cross)[0])\n",
    "# print(\"The peasorn of GradientBoostingRegression:\", pearsonr(Y_pred, y_cross)[0])\n",
    "# print(\"The Peasorn of Xgboost:\", pearsonr(xg_pred, y_cross)[0])\n",
    "# print(\"The mse of all:\",mean_squared_error((Y_pred+y_pred+xg_pred)/3, y_cross))\n",
    "print(\"The peasorn of all:\", pearsonr((Y_pred+Y_pred2+y_pred+y_pred2+xg_pred+xg_pred2+cv_pred+cv_pred2)/4, y_cross[:,2])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"submission_sample.txt\", \"w\") as f:\n",
    "    for i in range(550):\n",
    "        f.write(str(df_test.id[i]) + \",\" + str((final1_1[i]+final1_2[i]+final1_3[i]+final2_1[i]+final2_2[i]+final2_3[i])/3) + \"\\n\")\n",
    "    #         # f.write(y_ids[i] + \",\" + str(y_pred[i]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
